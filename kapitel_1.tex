
\chapter{Mathematische\ Modellierung\ des\ Zufalls}
\section{Zufallsexperimente}
\subsection{Das Würfeln}
\Vorlesung{14.4.2010}y
$\Omega = \lbrace 1, ... , n  \rbrace , n \in \mathbb{N}$ Wir möchten zufällig genau eine Zahl aus $\Omega$ ziehen. Eine Möglichkeit: n-seitiger Würfel\newline
Ansatz: $Pr[i] = \frac{1}{n}  \forall i \in \Omega$\newline
$Pr \mathrel{\widehat{=}} ``Probability''$\newline
Sei $A=\lbrace a_1, ... , a_k \rbrace \subset \Omega$, dann ist $Pr[A]=\frac{\left|A\right|}{n} = \frac{k}{n}$ die Wahrscheinlichkeit, dass $a_1, ..., a_{k-1}$ oder $a_{k}$ ausgewählt werden.\newline
A nennt man \Begriff{Ereignis}\newline
Wenn alle $Pr[i]$ gleich sind so spricht man von einer \Begriff{Gleichverteilung}.\newline
Bei Spielen: n=6
Es herrscht Unabhängigkeit der Würfe, d.h. Ergebnisse beeinflussen sich nicht.

\subsection{Das Lottospiel}
Es werden 6 Zahlen aus 49 gezogen, sagen wir $a_1, .. a_6$. Wir nehmen an, dass wir diese schon geordnet haben: $a_1 < ... < a_6$. Eine Ziehung ist ein Vektor $(a_1,...,a_6)$ mit $a_1 < ... < a_6$. Ergebnisse sind diese Vektoren. Man fasst die Ergebnisse zu einem \Begriff{Grundraum} zusammen, den wir üblicherweise $\Omega$ nennen.
$$ \Omega = \lbrace \lbrace a_1,...,a_6 \rbrace \mid a_i \in \lbrace 1,...,49 \rbrace \forall i=1...6  \rbrace$$
$$ \left|\Omega\right| = \binom{49}{6} = 13983816$$%
Wie hoch ist die Wahrscheinlichkeit, dass ein Tipp 6 Richtige hat? Allgemeiner: k Richtige? %
Welche $\lbrace a_1,...,a_6 \rbrace \in \Omega$ haben $k$ Stellen gemeinsam mit dem Tipp? %
Die bezeichnen wir als \Begriff{günstige\ Ereignisse}.\newpage%

$$A = \lbrace \lbrace a_1,...,a_6 \rbrace \in \Omega \mid \left| \lbrace a_1,...,a_6 \rbrace \bigcup \lbrace b_1,...,b_6 \rbrace \right| = k \rbrace$$%

Wenn ein Element aus $A$ gezogen wird haben wir $k$ Richtige.%
$$ Pr[k \textrm{Richtige}]=\frac{\left| A \right|}{\left| \Omega \right|} = \frac{\binom{6}{k} \cdot \binom{43}{6-k}}{\binom{49}{6}}$$
$$\Rightarrow Pr[6 \textrm{Richtige}]= \frac{\binom{6}{6} \binom{43}{0}}{\binom{49}{6}}=\binom{1}{49}$$
\Vorlesung{19.4.2010}
\subsection{Das Geburtstagsproblem}
Wie hoch ist die Wahrscheinlichkeit, dass zwei Personen in einer Gruppe von $m$ Personen am gleichen Tag Geburtstag haben?
z.B.: $m=50$
% Abb 1
\subsection{Das Ziegenproblem}
Bei einem Fernsehquiz gibt es drei Türen, wobei hinter zwei Türen eine Ziege steht und hinter der dritten Tür ein Hauptgewinn. %
Der Showmaster fragt den Kandidaten, welche Tür er öffnen soll. Nachdem der Kandidat einen Tipp abgegeben hat, bietet der Showmaster seine Hilfe an: %
Er schlägt vor eine Tür zu öffnen und danach soll der Kandidat die Möglichkeit haben seinen Tipp zu verändern.%
Würden Sie sein Angebot annehmen?
% Abb 2
\begin{quote}
 In diesem Fall ist es nicht so einfach, da die günstigen Ereignisse nicht unabhängig sind.
\end{quote}
\section{Algebra und Maße}
Die axiomatische Fundierung der Wahrscheinlichkeitsrechnung wurder von Kolmogoroff Anfang des 20. Jahrhunderts gegeben. Erst dadurch wurde Wahrscheinlichkeitsrechnung eine wissenschaftliche und quantitative Disziplin.
\subsection{Die Axiome von Komolgoroff}
\begin{itemize}
 \item Ergebnisse eines Zufallsexperimentes fassen wir in einer Menge $\Omega$ zusammen.
 \item \Begriff{Ereignisse} sind gewisse Teilmengen von $\Omega$.
 \item Wahrscheinlichkeiten werden auf Ereignisse bezogen und sind Zahlen zwischen 0 und 1. Eine Zahl $p \in [0,1]$,z.B. $p=\frac{3}{4}$ besagt dass das entsprechende Ereigniss in $\frac{3}{4}$ der Zufallsexperimente auftrit, oder die Wahrscheinlichkeit ist 75 Prozent.

$p=0$: Das Ereignis tritt nicht auf.

$p=1$: Das Ereignis tritt stets auf.
\end{itemize}
Beispiel: Wir werfen eine faire Münze. Kopf und Zahl kommen mit der gleichen Wahrscheinlichkeit.
$$ \textrm{Menge } \Omega = \{\textrm{Kopf},\textrm{Zahl}\} = \{0,1\} $$
$$ \textrm{Ereignisse} = \{\{0\},\{1\},\emptyset,\{0,1\}\} = \mathcal P(\Omega) = \textrm{Potenzmenge von } \Omega $$
$$ P(\{0\})=\frac{1}{2}=P(\{1\}), P(\emptyset)=0 , P(\{0,1\})=1$$
\subsection{Erweiterung der Kolmogoroff-Axiome}

Wie muss die Menge der Ereignisse aussehen?
\begin{itemize}
 \item $A$ Ereignis mit Wahrscheinlichkeit $p$. Wir möchten das Gegenereignis (``$A$ tritt nicht auf'' = $A^c$) ebenfalls als Ereignis ansehen mit der Wahrscheinlichkeit $1-p$.
 \item Ereignisse $A$,$B$ mit den Wahrscheinlichkeiten $p$ und $q$ . Was ist das Ereignis ``$A$ oder $B$'' bzw. ?

 ``$A$ oder $B$'' = $A \cup B$

 ``$A$ und $B$'' = $A \cap B$

 Mithin sollten auch diese Ereignisse in der Ereignismenge drin sein.
 \item Was ist $P(A \cup B)$?

Situation: $A \cap B = \emptyset$ dann ist $P(A \cup B) = P(A) + P(B)$

\end{itemize}
\subsection{Sigma-Algebra}
\begin{definition}
Es sei $\Omega$ eine Menge und $\Sigma \subseteq \mathcal P(\Omega)$. $\Sigma$ heißt $\sigma$-Algebraüber $\Omega$ fallse folgende Eigenschaften gelten:
\end{definition}
\begin{quote}
 Das $\sigma$ steht für die Abzählbarkeit der Vereinigung. Für bestimmte Modellierungen ist die Potenzmenge zu groß, daher beschränkt man sich auf eine Teilmenge.
\end{quote}

\begin{itemize}
 \item $\Omega, \emptyset \in \Sigma$
 \item $A \in \Sigma \Rightarrow A^c \in \Sigma$
 \item Für eine Folge $A_1,A_2,... \in \Sigma$ gilt:
$$ \bigcup_{i=1}^\infty A_i \in \Sigma$$ ( $\Sigma$-Abgeschlossenheit )
\end{itemize}
Ereignisse über einer Menge $\Omega$ werden in der Wahrscheinlichkeitstheorie durch eine geeignete $\sigma$-Algebra beschrieben wird.

Was ist mit $A \cap B$ falls $A,B \in \Sigma$? Ist $A \cap B \in \Sigma$?
$$ A \cap B = (A^c \cup B^c)^c$$
$$ \Rightarrow A^c, B^c \in \Sigma \Rightarrow A^c \cup B^c \in \Sigma \Rightarrow (A^c \cup B^c)^c \in \Sigma $$

$\{\emptyset, \Omega\}$ ist die kleinste und $\mathcal P(\Omega)$ ist die größte $\sigma$-Algebra über $\Omega$.
\Vorlesung{21.4.2010}
\begin{example}
Sei $\mathcal F$ die Menge der abgeschlossenen Intervalle der Form $[a,b] \in \mathbb R^d; a,b \in \mathbb R^d$
$$ [a,b]=[a_1,b_1] \times ... \times [a_d,b_d], a=(a_1,...,a_d), b=(b_1	,...b_d)$$

Für d=1: $[a,b]$

Für d=2: 

% Rechteck
Für d=3: 

% Würfeln
\end{example}
$\mathcal B^d$ sei die kleinste $\sigma$-Algebra über $\mathbb R^d$, die $\mathcal F$ enthält. Unter kleinste verstehen wir, dass jede $\sigma$-Umgebung $\Sigma$ mit $\Sigma \supset \mathcal F$ und $\Sigma \subseteq \mathcal{B}^d$ $\Sigma = \mathcal{B}^d$ erfüllt.

Warum existiert $\mathcal{B}^d$?
\begin{quote}
 Dann hat es auch keinen weiteren Grund, dass ich diese Vorlesung halte.
\end{quote}
\begin{quote}
 $\mathcal{B}^d$ existiert, da mit $\mathcal P(\mathbb R^d) \supseteq \mathcal F$ eine $\sigma$-Algebra existiert. Somit existiert auch eine minimale $\sigma$-Algebra.
\end{quote}
Sei $M=\{\Sigma | \Sigma ist \sigma-\textrm{Algebra über} \mathbb R^d, \mathcal F \subseteq \Sigma\}$.
Suche die kleinste $\sigma$-Algebra aus $M$. Das geht wenn $M \neq \emptyset$. Da $\mathcal P(\mathbb R^d) \in M$ ist dies gegeben.
Definition 1.2 $\mathcal B^d$ nennt man die \Begriff{Borelsche $\sigma$-Algebra} über $\mathbb{R}^d$.
Wie sehen die Mengen in $\mathcal B^d$ aus?

Für eine Menge $X \subseteq \mathcal P(\Omega)$ verstehen wir unter $\sigma(X)$ die kleinster $\sigma$-Algebra, die $X$ enthält. Sie $\mathcal O$ die Menge der offenen Intervall in $\mathbb R^d$. Betrachte $\sigma(\mathcal O)$.

\begin{theorem}
$$ X \subseteq \mathcal P(\Omega); Y \subseteq \mathcal P(\Omega), X \subseteq Y \Rightarrow$$
$$ \sigma(X) \subseteq \sigma(Y)$$
\end{theorem}
\begin{proof}
Nehme $Z \in \sigma(X) \Rightarrow Z \in \sigma(Y)$.

Ausgangssituation:
$\Omega$ ist gegeben. $X$ ist Menge von Teilmengen von $\Omega$ d.h. $X \subseteq \mathcal P(\Omega)$.
Dann ist $\sigma(X)$ die kleinste $\sigma$-Algebra über $\Omega$, die $X$ enthält, d.h. $X \in \sigma(X)$.
\end{proof}

\Vorlesung{26.4.2010}
$\sigma(\mathcal O)$ ist die kleinste $\sigma$-Algebra, die die Menge $\mathcal O$ der offenen Intervalle in $\mathbb R^d$ enthält.

\begin{theorem}
$\mathcal B^d = \sigma(\mathcal O)$
\end{theorem}

\begin{proof}
Zur Vereinfachung sei $d=1$.
\begin{itemize}
 \item[Zunächst $\mathcal B^d \subseteq \sigma(\mathcal O)$:]
Wir wissen, dass $\mathcal B^d = \sigma(\mathcal F)$ ist. $\mathcal F$ ist Erzeuger von $\mathcal B^d$.
\begin{quote}
 Bei der $\sigma$-Algebra reicht es zu Zeigen, dass der Erzeuger in der Menge enthalten ist.
\end{quote}
Falls $\mathcal F \subseteq^{(*)} \sigma(\mathcal O)$ dann gilt $\sigma( \mathcal F) \subseteq \sigma( \sigma( \mathcal O)) = \sigma(\mathcal(O))$.
\begin{quote}
 $\sigma(\sigma(M)) = \sigma(M)$ da $\sigma (M)$ bereits eine $\sigma$-Algebra ist.
\end{quote}
Das heißt: es reicht $(*)$ zu zeigen. Sei $[a,b] in \mathcal F$ ein beliebiges geschlossenes Interval. Wie können wir $[a,b]$ mit Hilfe von offenen Intervallen oder anderen Elementen aus $\sigma(\mathcal O)$ erzeugen?

Vorschlag:
$$ [a,b] = ( ]-\infty, a[ \cup ]b,\infty[ )^c$$
\begin{quote}
 Schon nicht schlecht, aber die Frage ist, ob $]-\infty, a[ \in \sigma(\mathcal O)$ ist. Denn bisher haben wir die offenen Intervalle die Form $]x,y[; x,y \in \mathbb R$ und $\infty \notin \mathbb R$.
\end{quote}

Vorschlag2:
$$ \sigma(\mathcal O) \ni \bigcup_{n=1}^\infty ]b,b+n[ = ]b,\infty[ $$
\begin{quote}
 Da dies eine abzählbare Vereinigung ist von Mengen aus $\sigma(\mathcal O)$ ist auch die Vereinigung wieder aus $\sigma(\mathcal O)$. Somit ist der Vorschlag brauchbar.
\end{quote}
Folglich ist jedes Element auf $\mathcal F$ auch in $\sigma(\mathcal O)$.
\item[Nun $\sigma(\mathcal O) \subseteq \mathcal B^d$:]
Es reicht zu zeigen, dass $\mathcal O \subseteq \mathcal B^d$ ( wie eben ). Sei $]a,b[ \in \mathcal O$ beliebig und $a,b \in \mathbb R$. Zu zeigen $]a,b[ \in \mathcal B^d$.

Vorschlag:
$$ \mathcal B^d \ni \bigcup_{n=1}^\infty [a+\frac{1}{n},b-\frac{1}{n}] = ]a,b[ $$
\begin{quote}
 Wieder handelt es sich um eine abzählbare Vereinigung von Elementen aus $\mathcal B^d$. Die Grenzen $a$ und $b$ sind natürlich nicht in der Vereinigung, aber man kann für jede Zahl $c$ zwischen $a$ und $b$ ein $n$ finden sodass $c$ in der Vereinigung ist.
\end{quote}
\end{itemize}

Wir wissen:

$$ \mathcal B ^d \subseteq \mathcal P (\mathbb R^d) $$
Frage:
Gibt es Mengen $A \subseteq \mathbb R^d$ mit $A \neq \mathcal B^d$? bzw. $ \mathcal B ^d = \mathcal P (\mathbb R^d) $?

Vorschlag:
$$ \mathbb R^d \supset A = \bigcup_{a\in A} \lbrack a \rbrack \in \mathcal B^d $$
Die Vereinigung ist nur in $\mathcal B^d$ falls $A$ abzählbar ist.

Es gibt Teilmengen des $\mathbb R^d$ die keine Borelmengen sind!
$$ \mathcal B ^d \subsetneq \mathcal P (\mathbb R^d) $$
Diese nennt man \Begriff{Cantor-Mengen}.
\end{proof}


Um die Wahrscheinlichkeit eines Ereignisses zu definieren möchten wir - wenn Ereignisse durch Elemente einer $\sigma$-Algebra beschreiben werde, also Teilmengen einer Grundmenge $\Omega$ sind - diesen Teilmengen eine Maßzahl zuordnen, d.h. sie messen.
\begin{itemize}
 \item[1.Fall]
$$ \lvert \Omega \rvert < \infty, A \subseteq \Omega$$
$\lvert A \rvert$ ist ein Maß. Bei Gleichverteilung ergibt $\frac{\lvert A \rvert}{\lvert \Omega \rvert}$.
 \item[2.Fall]
$$ \lvert \Omega \rvert = \infty, A \subseteq \Omega$$
Falls $\lvert A \rvert = \infty$ so ist ein Maß der Form $\lvert A \rvert$ zu undifferenziert.
\end{itemize}
\section{Wahrscheinlichkeitsraum}
Ist $\Sigma$ eine $\sigma$-Algebra über $\Omega$, so nennt man $(\Omega,\Sigma)$ einen \Begriff{Meßraum}.

\begin{definition} Sei $(\Omega,\Sigma)$ ein Meßraum.
\begin{enumerate}
 \item Eine Funktion $\mu: \Sigma \rightarrow [0,\infty]$ heißt \Begriff{Maß} falls folgendes gilt:
  \begin{enumerate}
   \item $\mu(\emptyset) = 0$

   \item für jede Folge von paarweise disjunkten Mengen $A_1, A_2, ... \in \Sigma$ gilt:
$$ \mu \left ( \bigcup_{i=1}^\infty A_i \right ) = \sum_{i=1}^\infty \mu \left ( A_i \right ) $$
  \end{enumerate}
 \item Ein Maß $P: \Sigma \rightarrow [0,1]$ mit $P(\Omega) = 1$ heißt \Begriff{Wahrscheinlichkeitsmaß}.
 \item Das Tripel $(\Omega,\Sigma,P)$ nennt man \Begriff{Wahrscheinlichkeitsraum}
\end{enumerate}
\end{definition}

Zusammenhang zur ``experimentellen'' oder ``intuitiven'' Wahrscheinlichkeit:

\subsection{Komolgoroff-Axiome}
Ein Zufallsexperiment wird im mathematischen Sinne durch einen Wahrscheinlichkeitsraum $(\Omega,\Sigma,P)$ beschrieben. Dabei gilt:
\begin{enumerate}
 \item $\Omega$ umfasst alle Ergebnisse $\omega \in \Omega$ des Zufallsexperimentes.
 \item Die Ereignisse sind die Mengen aus $\Sigma$.
 \item Für eine Menge $A \in \Sigma$ ist $P(A)$ die Wahrscheinlichkeit des Ereignisses $A$.
\end{enumerate}

\Vorlesung{28.4.2010}
\subsection{Eigenschaften eines Wahrscheinlichkeitsmaßes}
\begin{proposition}
 Sei $(\Omega,\Sigma,P)$ ein Wahrscheinlichkeitsraum. Es seien $A,B,A_1,A_2, ... \in \Sigma$. Dann gilt:
 \begin{enumerate}
  \item $P(A^c) = 1 - P(A)$
  \item $P( A \cup B ) + P( A \cap B ) = P(A) + P(B)$
  \item Aus $A \subseteq B$ folgt $P(B \setminus A) = P(B) - P(A)$
  \item Aus $A \subseteq B$ folgt $P(A) \leq P(B)$
  \item Für $A_1, ... A_n \in \Sigma$ gilt:
$$ P \left ( \bigcup_{i=1}^n A_i \right ) \leq \sum_{i=1} P(A_i)$$ ( Union-Bound )
 \end{enumerate}
\end{proposition}
\begin{proof}

 \begin{enumerate}
  \item 
\begin{quote}
 Wir wissen schon, dass $P(\Omega) = 1$ gilt. Folglich ist zu zeigen, dass $ P(A^c) + P(A) = 1 = P(\Omega)$ gilt.
\end{quote}
\begin{equation}
 P(\Omega)
 = P(A \cup A^c)
 = P(A) + P(A^c)
\end{equation}
\begin{quote}
 Der letzte Schritt ist möglich, da $A$ und $A^c$ paarweise disjunkt sind.
\end{quote}
  \item
$$ A \cup B = A \cup ( B \setminus A ) $$%
Daraus folgt aufgrund der $\sigma$-Additivität:%
$$ P(A \cup B) = P(A) + P( B \setminus A ) $$%
Weiterhin gilt:%
$$ B = B \setminus A \cup (A \cap B) \Rightarrow P(B) = P(B \setminus A) + P(A \cap B)$$
$$ \Rightarrow P(B \setminus A) = P(B) - P(A \cap B) $$
$$ \Rightarrow P(A \cup B) = P(A) + P(B) - P(A \cap B) $$
  \item
$$ B = A \dot{\cup} (B \setminus A) $$
$$ \Rightarrow P(B) = P(A) + P(B \setminus A) $$
$$ \Rightarrow P(B \setminus A) =  P(B) - P(A) $$
  \item 
$$ B = A \dot{\cup} (B \setminus A) $$
$$ \Rightarrow P(B) = P(A) + P(B \setminus A) \geq P(A) $$
  \item
\begin{quote}
 Für $n=2$ ist die Behauptung schon gezeigt.
\end{quote}
Versuch Darstellung als:
$$ \bigcup_{i=1}^n A_i =  \bigcup_{i=1}^l B_i $$
mit $B_i$ paarweise disjunkt.
$$ B_1 = A_1 $$
$$ B_2 = A_2 \setminus A_1 $$
$$ B_3 = A_3 \setminus ( A_1 \cup A_2) $$
$$ B_n = A_n \setminus ( A_1 \cup ... \cup A_{n-1} ) $$
Die $B_i$ sind immer paarweise disjunkt und $B_i \subseteq A_i ; i \in 1,...,n$ so wie
$$ \bigcup_{i=1}^n A_i =  \bigcup_{i=1}^n B_i $$
$$ \Rightarrow P \left ( \bigcup_{i=1}^n A_i \right ) = P \left ( \bigcup_{i=1}^n B_i \right ) = \sum_{i=1}^n P(B_i) $$
Nun wissen wir aber nach Konstruktion, dass jedes $B_i \subseteq A_i$ und somit $P(B_i) \leq P(A_i)$.
Daraus folgt:
$$ \sum_{i=1}^n P(B_i) \leq \sum_{i=1}^n P(A_i) $$
 \end{enumerate}
\end{proof}

\subsection{Der Laplace-Raum}
Sei $\Omega$ endllich. Sei $\Sigma = \mathcal P(\Omega)$ und $P: \Sigma \rightarrow [0,1], P(A):= \frac{\lvert A \rvert}{\lvert \Omega \rvert}$ für alle $A \in \Omega$. %
$(\Omega, \Sigma)$ ist ein Meßraum, weil $\Sigma = \mathcal P(\Omega)$ eine $\sigma$-Algebra.
$P$ ist ein Wahrscheinlichkeitsmaß, da:
\begin{enumerate}
 \item $P(A) \in [0,1] \forall A \subseteq \Omega$
 \item $P(\emptyset) = \frac{\lvert A \rvert}{\lvert \Omega \rvert} = \frac{0}{\lvert \Omega \rvert} = 0$
 \item $P(\Omega) = \frac{\lvert \Omega \rvert}{\lvert \Omega \rvert} = \frac{\lvert \Omega \rvert}{\lvert \Omega \rvert} = 1$
 \item $A_1,A_2,... A_n \subseteq \Omega$ paarweise disjunkt.
$$ P(A_1 \cup ... \cup A_n) = \frac{\lvert A_1 \cup ... \cup A_n \rvert}{\lvert \Omega \rvert} = \frac{\lvert A_1 \rvert + ... \lvert A_n \rvert}{\lvert \Omega \rvert} = $$
$$ \frac{\lvert A_1 \rvert}{\lvert \Omega \rvert} + ... +  \frac{\lvert A_n \rvert}{\lvert \Omega \rvert} = P(A_1) + ... + P(A_n) = \sum_{i=1}^n P(A_i) $$
\begin{quote}
 Da $\Omega$ endlich ist, gibt es auch nur endlich viele verschieden Teilmengen.
\end{quote}

Mit ``Übergang'' aus $n \rightarrow \infty$ ist $P$ $\sigma$-Additiv, also ein Wahrscheinlichkeitsmaß.
\end{enumerate}
Also ist $(\Omega,\Sigma,P)$ ein Wahrscheinlichkeitsraum.


\begin{enumerate}
 \item 
Gibt es ein Wahrscheinlichkeitsmaß $P$ auf $\mathbb N$ mit $\mathcal P(\mathbb N)$ als $\sigma$-Algebra, so dass $P(\{i\}) > 0 \forall i \in \mathbb N$?
 \item
Gibt es ein Wahrscheinlichkeitsmaß $P$ auf $\mathbb R$ mit geeigneter $\sigma$-Algebra, die alle einelementigen Mengen $\{x\}; x \in \mathbb R$ enthält und %
$P(\{x\}) > 0$?
\end{enumerate}
\paragraph*{Zu 1:}
$P({i}) = \frac{1}{2^i}; i = 1,2,... $ (geometrische Folge). Dann ist
$$P(\mathbb N) = P \left ( \bigcup_{i=1}^\infty \{i\} \right ) = \sum_{i=1}^\infty \frac{1}{2^i} %
= \frac{1}{1-\frac{1}{2}} - 1 = 1$$ 
Wie sieht dann $P(A)$ für $A \subseteq \mathbb N$ aus?
$$P(A) = \sum_{i \in A} P(\{i\})$$
$$ \Rightarrow P(\emptyset) = 0 \textrm{ da die Leere Summe 0 ist.}$$
Wir überzeugen uns, dass ein $P$ ein Wahrscheinlichkeitsmaß ist:
\begin{enumerate}
 \item $ P(\Omega) = P(\mathbb N) = 1 $
 \item Sei $ A_1, A_2, ... A_n \subseteq \Omega \textrm{ paarweise disjunkt } $ dann gilt:
\begin{eqnarray*}
 P(A_1 \cup ... \cup A_n) & = & \sum_{i \in A_1 \cup ... \cup A_n} P(\{i\}) \\
 & = & \sum_{i \in A_1} P(\{i\}) + ... + \sum_{i \in A_n} P(\{i\}) \\
 & = & P(A_1) + ... + P(A_n) \nonumber
\end{eqnarray*}
Folglich ist $P$ ein Wahrscheinlichkeitsmaß.
\end{enumerate}

\subsection{Lebesquemaß}
\Vorlesung{3.5.2010}
Als wichtiges Beispiel betrachten wir das \Begriff{Lebesguemaß}, das einerseits unser natürliches Verständnis des "Messens" von %
regulären Mengeninhalten in $\mathbb R^d$ widerspigelt, aber gleichzeitig durch die Nullmengeneigenschaft unser Intuition %
zuwiderläuft. Diese Nullmengeneigenschaft ist aber charackteristisch und grundlegend zum Verständnis der Wahrscheinlichkeitstheorie, %
auch im diskreten Fall.
Sei $\lbrack a,b \rbrack \in \mathbb R^d$ ein Intervall, für $a=(a_1, ... , a_d )$ und $b=(b_1, ... , b_d)$ sei
$$ \lbrack a,b, \rbrack = \lbrack a_1,b_1 \rbrack \times ... \times \lbrack a_d,b_d \rbrack $$
Das Volumen von $\lbrack a,b \rbrack$ ist offenbar $(b_1 - a_1) \cdot ... \cdot (b_d - a_d)$. Wir schreiben $Vol(\lbrack a,b \rbrack)$ %
für das Volumen von $\lbrack a,b \rbrack$.

Eine grundl Frage der Maptheorie ist die Frage:
Gibt es ein Maß für Borelmengen $\mathcal B^d$ in $\mathbb R^d$, das für $\lbrack a,b \rbrack$ in $\mathbb R^d$ genau das Volumen angibt? %
Diese Frage kann positiv beantwortet werden und ist ein grundlegendes Resultat der Maßtheorie. Wir notieren (ohne Beweiß):
\begin{theorem}
Für jedes $d\in \mathbb N $ gibt es ein Maß $\lambda^d, \lambda^d: \mathcal B^d \rightarrow \lbrack 0,\infty \lbrack$ %
mit der Eigenschaft $\lambda^d(\lbrack a,b\rbrack) = Vol(\lbrack a,b \rbrack)$ für jedes Intervall $\lbrack a,b \rbrack \in \mathbb R^d$.
\end{theorem}

\begin{definition}
$\lambda^d$ heißt $d$-dimensionales Lebesguemaß. Wir nenne $\lambda = \lambda^d$ Lebesguemaß.
\end{definition}

\begin{definition}
Sie $(\Omega,\Sigma,\mu)$ ein Maßraum. Eine Menge $A \in \Sigma $ heißt $\mu$-Nullmenge, wenn $\mu(A)=0$.
\end{definition}

Für das Lebesguemaß haben wir:

\begin{theorem}
Jede abzählbare Menge in $\mathbb R^d$ ist eine Borelmenge und sogar eine Nullmenge bezüglich $\lambda$.
\end{theorem}

\begin{proof}
Sei $A \subseteq \mathbb R^d$ und $A=\{a_1,a_2, ...  \}$. Wegen $A=\bigcup\limits_{i=1}^\infty \{a_i\}$ ist $A$ aus $\mathcal B^d$ %
Wegen $\lambda^d(\{a_i\})=0$ folgt
$$ \lambda^d(A)=\sum\limits_{i=1}^\infty \lambda^d(\{a_i\}) = 0 $$
\begin{quote}
$\lambda^d(\{a_i\})=0$ gilt, weil eine einzelne Zahl kein Volumen hat.
\end{quote}
\end{proof}

Wenn man beachtet, dass $\mathcal B^d _{\lbrack 0,1\rbrack}$ eine $\sigma$-Algebra über $\lbrack 0,1 \rbrack$ ist, so erhält man:
\begin{proposition}
$(\lbrack 0,1 \rbrack,\mathcal B^d _{\lbrack 0,1\rbrack}, \lambda^d)$ ist ein Wahrscheinlichkeitsraum.
\end{proposition}
Nillmengen spielen in der Wahrscheinlichkeitstheorie eine grundlegende Rolle, denn ein Ereignis mit der Wahrscheinlichkeit %
0 tritt nicht auf.

\section{Bedingte Wahrscheinlichkeiten}
Ereignisse können sich in vielfacher Weise beeinflussen. Es ist daher von grundlegender Bedeutung, der Einfluss eines %
Ereignisses auf das auftreten eines anderen Ereignisses mathematisch zu formulieren.
\begin{definition}[bedingte Wahrscheinlichkeit]
Sei $(\Omega,\Sigma,P)$ ein Wahrscheinlichkeitsraum und $A,B \in \Sigma$ mit $P(B) > 0$. Der Ausdruck
$$ P(A|B) = \frac{P(A\cap B)}{P(B)} $$
ist die \Begriff{bedingte Wahrscheinlichkeit} von A gegeben B.
\end{definition}
Ein besonderer Fall liegt vor, wenn $P(A \cap B)=P(A) \cdot P(B)$ ist. Dann gilt:
$$ P(A|B) = \frac{P(A\cap B)}{P(B)} = \frac{P(A) \cdot P(B)}{P(B)} = P(A) $$.
d.h. $B$ hat keinen Einfluss auf $A$. Umgangssprachlich sagen wir, dass $A$ von $B$ unabhängig ist. Der Begriff der %
\Begriff{Unabhängigkeit} ist ein grundlegendes Konzept, das wir später definieren werde.
Mithilfe der bedingten Wahrscheinlichkeit können wir $P(A \cap B)$ für beliebige Ereignisse $A,B \in \Sigma$ %
( mit $P(B) > 0$ ) berechnen. Gemäß Def(1.12) gilt:
$$ P(A \cap B) = P( A|B ) \cdot P(B) \text{1.4} $$
Falls $A$ und $B$ unabhängig sind, vereinfacht sich diese Gleichung wieder zu $P(A \cap B) = P(A) \cdot P(B)$, %
das heißt sie enthält den Fall "unabhängige Ereignisse" als wichtigen Spezialfall. Gleichung 1.4 lässt sich für mehr %
als zwei Ereignisse verallgemeinern (siehe nächster Satz).

Wenn $P(A \cap B) = P(A) \cdot P(B)$, dann $P(A|B) = P(A)$. Dann ist A "unabhängig" von B.

Wenn $A \cap B = \emptyset$, dann ist $P(A \cap B)=P(\emptyset)=0$.

\begin{proposition}[Multiplikationssatz]
Seien $A_1,...,A_n$ Ereignisse mit $P \left ( \bigcap\limits_{i=1}^{n-1} A_i \right ) > 0$. Dann gilt:
$$ P \left ( \bigcap\limits_{i=1}^{n} A_i \right ) = P(A_1) \cdot P(A_2 | A_1) \cdot P(A_3 | A_1 \cap A_2 ) \cdot ... \cdot P \left ( A_n | \bigcap\limits_{i=1}^{n-1} A_i \right ) $$
\end{proposition}
\begin{proof}
Wegen $P(A_1) => P( A_1 \cap A_2 ) => .. =>  P \left ( \bigcap\limits_{i=1}^{n-1} A_i \right ) > 0$ sind alle bedingten %
Wahrscheinlichkeiten wohldefiniert. Es gilt 
\begin{eqnarray*}
P(A_1) \cdot P(A_2 | A_1) \cdot P(A_3 | A_1 \cap A_2 ) \cdot ... \cdot P \left ( A_n | \bigcap\limits_{i=1}^{n-1} A_i \right ) & = & \frac{P(A_1)}{1} \cdot \frac{P(A_1 \cap A_2)}{P(A_1)} \cdot ... \cdot \frac{P(A_1 \cap ... \cap A_n)}{P(A_1 \cap ... \cap A_{n-1})}
\cr & = & P(A_1 \cap ... \cap A_n)
\end{eqnarray*}
\end{proof}
\begin{theorem}[Satz der totalen Wahrscheinlichkeit]
Die Ereignisse $B_1,...,B_n$ sind paarweise disjunkt mit $P(B_i) > 0$ für $i=1,...,n$ und es gelte $\Omega = B_1 \cup ... \cup B_n$. %
Dann folgt:
$$ P(A) = \sum\limits_{i=1}^{n} P(A|B_i) \cdot P(B_i) \forall A \in \Sigma$$
Motivation: man möchte $P(A)$ berechnen. $A$ liegt jedoch in einer Weise vor, dass wir $P(A)$ nicht direkt berechnen können. %
Falls alle $P(A \cap B_i)$ leichter zu berechnen wären als $P(A)$, so könnte man mit Satz 1.15 $P(A)$ leicht berechnen.
\end{theorem}
\Vorlesung{5.5.2010}
\begin{proof}
\begin{align*}
\sum\limits_{i=1}^n P(A|B_i) \cdot P(B_i) & = \sum\limits_{i=1}^n P(A \cap B_i) \\ %
	& = P \left( \bigcup\limits_{i=1}^n ( A \cap B_i ) \right ) \\ %
	& = P \left( A \cap \left( \bigcup\limits_{i=1}^n B_i \right) \right ) \\ %
	& = P ( A \cap \Omega ) \\ %
	& = P(A)
\end{align*}
\end{proof}
\subsection{Lösung des Ziegenproblems}
Betrachte geeignete Ereignisse:
\begin{align*}
A &:= \text{Kandidatin hat bei erster Wahl das Auto gewählt} \\ %
G &:= \text{Kandidatin gewinnt nach dem Wechsel der Tür das Auto}
\end{align*}
Uns interessiert $P(G)$ im Vergleich zu $P(A)$. Falls der Tip ohne Hilfe abgegeben wurde, so ist $P(A)=\frac{1}{3}$.
\begin{quote}
Nun haben wir eine Situation in der wir $P(G)$ nicht direkt berechnen können. Aber wir können folgendes mit Satz 1.15 machen:\end{quote}
\begin{align*}
P(G)=P(G|A)\cdot P(A) + P(G|A^c)\cdot P(A^c)
\end{align*}
Nun ist $P(G|A)=0$ und somit $P(G|A)\cdot P(A) = 0$. Weiterhin ist $P(G|A^c)=1$ und $P(A^c)=\frac{2}{3}$ und somit %
$P(G|A^c)\cdot P(A^c) = \frac{2}{3}$. Folglich ist $P(G) = \frac{2}{3} > P(A)$.

\subsection{Lösung des Geburtstagsproblems}
\begin{quote}
Dieses Problem gehen wir mit der Proposition 1.14 an.
\end{quote}
Modelliere das Problem mit Hilfe von Urnen. Wir haben $n$ Urnen ( z.B. $n = 365$ ). Wir werfen jeden der $m$ Bälle ( z.B. %
Personen ) zufällig unter Gleichverteilung in die Urnen. Wir setzen vorraus $m \leq n$. Die Wahrscheinlichkeit, dass ein %
Ball in die $i$-te Urne fällt ist $\frac{1}{n}$.
\begin{quote}
Wir nehmen an, dass die Wahrscheinlichkeit an einem bestimmten Tag Geburtstag zu haben nicht vom Tag abhängt.
\end{quote}
$A$ ist das Ereignis, dass alle Bälle in unterschiedlichen Urnen landen. $A^c$ ist das gewünschte Ereignis, nämlich %
dass in mindestens einer Urne mehr als ein Ball landet.
Wir berechnen $P(A^c)$ über $P(A^c)=1-P(A)$ und einer Abschätzung des schlechten Ereignisses $P(A) \leq \alpha$, was zu %
$P(A^c) = 1-P(A) \geq 1-\alpha$ führt.
Sei:
\begin{align*}
A_i &:= \text{Ball } i \text{ landet in einer noch leeren Urne}
\end{align*}
dann ist:
\begin{align*}
P(A) &= P \left ( \bigcap\limits_{i=1}^m A_i \right )
\end{align*}
Dann können wir den multiplikationssatz anwenden.
\begin{align*}
P \left ( \bigcap\limits_{i=1}^m A_i \right ) &= P(A_i) \cdot P(A_2|A_1) \cdot ... \cdot P(A_m|\bigcap\limits_{i=1}^{n-1} A_i )
\end{align*}
Schätze jeden Faktor nach oben ab:

\Vorlesung{10.5.2010}
\section{Stochastische Unabhängigkeit}
\begin{definition}
Seien $A_1, ...  , A_n$ Ereignisse.
\begin{enumerate}
\item Für ein festes $k \in \mathbb N, 2 \leq k \leq n$ heißen $A_1 , ..., A_n$ $k$-weise unabhängig, fallls für jede Auswahl %
$A_{i_1}, ... , A_{i_k}$ von $k$ Ereignissen aus den $A_1 , ... , A_n$ gilt:
\begin{align*}
P \left ( \bigcap\limits_{j=1}^k A_{i_j} \right ) = \prod\limits_{j=1}^k P(A_{i_j})
\end{align*}

\item Die Ereignisse $A_1, .. A_n $ heißen unabhängig ( \Begriff{stochastisch unabhängig} ), falls für jede Teilmenge $J \subseteq \{1,...n\}, J \neq \emptyset$ gilt:
\begin{align*}
P \left ( \bigcap\limits_{j \in J} A_{i_j} \right ) = \prod\limits_{j \in J} P(A_{i_j})
\end{align*}
\end{enumerate}
\end{definition}
\begin{quote}
Wenn $A_1, ..., A_n$ unabhängig ist, dann auch $k$-weise unabhängig für jedes $k \in \{2, ... , n\}$.
\end{quote}
Für $n=2$ spricht man von paarweiser Unabhängigkeit. Das bedeutet, dass aus $A_1,A_2, ... A_n$ je zwei unabhängig sind. %
Sind dann alle voneinander unabhängig?

\begin{example}
Sei $\Omega = \{1,2,3,4\}$. Betrachte das Experiment, aus $\Omega$ eine Zahl zufällig mit Wahrscheinlichkeit $\frac{1}{4}$ %
auszuwählen, unter Gleichverteilung.
\begin{itemize}
	\item $A$ sei das Ereignis: $1$ oder $2$ wird gewählt
	\item $B$ sei das Ereignis: $1$ oder $3$ wird gewählt
	\item $C$ sei das Ereignis: $1$ oder $4$ wird gewählt
\end{itemize}
\paragraph*{$2$-weise Unabhängigkeit}
\begin{align*}
P(A) P(B) &= \left( \frac{1}{4} + \frac{1}{4} \right)\left( \frac{1}{4} + \frac{1}{4} \right) \\
&=\frac{1}{4} \\
&= P(A\cap B)
\end{align*}
analog zerlegt man $P(A)P(C)$ und $P(B)P(C)$. Also ist paarweise unabhängigkeit gegeben.
\paragraph*{$3$-weise Unabhängigkeit}
\begin{align*}
P(A) P(B) P(C)&= \left( \frac{1}{4} + \frac{1}{4} \right)\left( \frac{1}{4} + \frac{1}{4} \right)\left( \frac{1}{4} + \frac{1}{4} \right) \\
&=\frac{1}{8} \\
&\neq \frac{1}{4} = P(A\cap B \cap C)
\end{align*}
Also keine $3$-weise Unabhängigkeit.
\end{example}

Hieraus folgt: Aus $k$-weiser Unabhängigkeit für ein festes $k$ folgt \textbf{nicht} notwendigerweise volle Unabhängigkeit. 

\begin{example}
Wir werfen einen Würfel zweimal hintereinander. Sei
\begin{itemize}
\item $A$ das Ereigniss, dass beim ersten Wurf eine gerade Zahl auftritt
\item $B$ das Ereigniss, dass beim zweiten Wurf eine gerade Zahl auftritt
\end{itemize}
$A$ und $B$ sind nach dem intuitiven Verständnis "unabhängig". Wir zeigen, dass sie auch im stochastischen Sinne %
unabhängig sind. Dazu muss das Experiment durch einen geeigneten Wahrscheinlichkeitsraum modelliert werden. %
Wähle $\Omega_1=\{1,2,3,4,5,6\}$ für den ersten Wurf und $\Omega_2$ ebenso für den zweiten Wurf.
\begin{quote}
Diese reichen nicht aus. Wir wollen ja nicht nur zwei einzelne Würfe betrachten, sondern eine Kombination aus zwei %
Würfen.
\end{quote}
Dann ist $\Omega = \Omega_1 \times \Omega_2$. $(i,j) \in \Omega$ repräsentiert, dass $i$ das Ergebnis des ersten Wurfes %
ist und $j$ das Ergebnis des zweiten Wurfes ist. Weiterhin gilt:
\begin{align*}
\Omega = \{ (i,j) : 1 \leq i,j \leq 6 \}
\end{align*}
Das Wahrscheinlichkeitsmaß $P$ wählen wir als die Gleichverteilung über $\Omega$ d.h.:
\begin{align*}
\mathbb P(\{(i,j)\}) = \frac{1}{\lvert \Omega \rvert} = \frac{1}{36} \forall (i,j) \in \Omega
\end{align*}
Wähle $\Sigma = \mathcal P(\Omega)$. Dann haben wir den Laplaceraum $(\Omega,\Sigma,\mathbb P)$. Daraus folgt: 
\begin{align*}
\mathbb P(A) &= \mathbb P ( \{(i,j) : 1\leq i,j \leq 6, i \text{ gerade} \}) \\
	&=\frac{\lvert A \rvert}{\lvert \Omega \rvert} \\
	&=\frac{18}{36} = \frac{1}{2}
\end{align*}
Analog dazu gilt $\mathbb P(B)= \frac{1}{2}$. D.h.:
\begin{align*}
\mathbb P(A)\mathbb P(B) &= \frac{1}{2} \frac{1}{2} = \frac{1}{4}
\end{align*}
Betrachte weiterhin:
\begin{align*}
A \cap B &= \{(i,j) : 1\leq i,j \leq 6, i,j \text{ gerade} \}
\lvert A \cap B \rvert &= 9
\mathbb P(A \cap B) &= \frac{\lvert A \cap B \rvert}{\lvert \Omega \rvert} \\
	&= \frac{9}{36} = \frac{1}{4}
\end{align*}
Also gilt:$\mathbb P(A)\mathbb P(B) = \mathbb P(A \cap B)$, d.h. $A$ und $B$ sind unabhängig.
\end{example}

\subsection{Informationstheoretischer Aspekt}
Betrachte $n$ Würfe einer fairen Münze. Dann gibt es unterschiedliche Möglichkeiten:
\begin{enumerate}
\item Die Würfe sind unabhängig voneinander
\item Je zwei Würfe sind unabhängig
\end{enumerate}
\begin{quote}
Der erste Möglichkeit ist einfach zu realisieren.
\end{quote}
Wieviele zufällige Bits benötigt man um diese Experimente zu beschreiben?
\begin{enumerate}
\item Man benötigt genau $n$ Bits.

\end{enumerate}

\Vorlesung{12.5.2010}
Ein endliches Problem $\Pi$ mit Instanzen $x \in \{0,1\}*$, Kodierungslänge $\lvert x \rvert =$ Anzahl der Komponenten %
von $x$, $\lvert x \rvert < \infty$. Ein Polynomzeitalgorithmus benötigt $\mathcal O(\lvert x \rvert ^ k)$ Schritte %
, $k \in \mathbb N$ fest abhängig von $\Pi$, um das Problem $\Pi$ mit Instanz $x$ zu lösen.
Lösen bedeutet hier: Entscheiden, ob $\Pi$ lösbar ist obder nicht, dabei ist $\Pi$ also ,,Ja``, ,,Nein`` Entscheidungs%
problem gegeben. z.B.: $p \in \mathbb N$, Frage: Ist $p$ eine Primzahl?

Zahlen $r \in \mathbb N$ bzw $r \in \mathbb Q$ werden binär kodiert : $\lvert r \rvert = \lceil log_2 r \rceil $ %
Für $r \in \mathbb Q, r = \frac{a}{b}, \lvert r \rvert := \lvert a \rvert + \lvert b \rvert$.

\begin{example}
Problem: Sei $G=(V,E)$ ein Graph mit Kantengewichten $w: E \rightarrow \mathbb N$.
Frage: Besitzt $G$ einen aufspannenden Baum mit Gewicht höchstens $W \in \mathbb N$?
Sei $\Pi$ obiges Problem. Wie ist die Kodierungslänge diese Problems.
$x$ sei Instanz mit $G=(V,E)$ und $w$.
\begin{align*}
\lvert x \rvert &= \lvert V \rvert + \lvert E \rvert + \sum\limits_{\varphi \in E} \lceil log_2 w(\varphi) \rceil
\end{align*}
\end{example}
\begin{remark}
Probleme werden eingeteilt in:
\end{remark}
\begin{itemize}
	\item[Die Klasse $\mathcal P$] Die Klasse der endlichen Probleme, die in Polynomialzeit lösbar sind.
	\item[Die Klasse $\mathcal{NP}$] Die Klasse der endlichen Probleme, deren Lösungen in Polynomialzeit verifizierbar %
sind
\end{itemize}
\subsection*{Schematische Darstellung von P und NP}
\begin{center}
	\includegraphics[scale=0.50]{kapitel1_darstellung_p.png}
	\captionof{figure}{Darstellung eines Algorithmus aus $\mathcal P$}
	\label{fig:p}
\end{center}

\begin{center}
	\includegraphics[scale=0.50]{kapitel1_darstellung_np.png}
	\captionof{figure}{Darstellung eines Algorithmus aus $\mathcal NP$}
	\label{fig:np}
\end{center}
\subsection*{Einbeziehung des Zufalls}
%\begin{center}
%	\includegraphics[scale=1.00]{kapitel1_darstellung_np_zufall.png}
%	\captionof{figure}{Darstellung eines Algorithmus aus $\mathcal NP$ unter Einbeziehung von Zufälligkeit}
%	\label{fig:np_zufall}
%\end{center}
Sei $\Pi$ ein Problem in $\mathcal NP$. Das Orakel gibt uns einen Beweis/Lösung $\pi$. Nehmen wir mal an, dass $\pi$ eine %
wirkliche Lösung ist. $\pi$ habe $l$ Stellen. Wieviele Stellen von $\pi$ muss man lesen um in polynomieller Zeit in %
$\lvert x \rvert$ zu verifizieren, dass $\pi$ wirklich eine Lösung ist?
Im Allgemeinen müssen wir alle $l$ Einträge von $\pi$ lesen! Dabei sollte gelten: $l=\mathcal O(\lvert x \rvert^k), k \text{ konstant}$.

Wieviele Stellen von $\pi$ muss ich lesen, wenn ich noch Random Bits $\tau$, $\lvert \tau \rvert = r$ zur Verfügung habe?
\begin{theorem}[Arora, Motwani, Lund, Szegdi '94; PCP-Theorem]
Man kann mit $log(\lvert x \rvert)$ Stellen von $\pi$ und konstant vielen Random Bits verifizieren, ob $\pi$ eine Lösung ist.
( Neue Characterisierung von $\mathcal{NP}$!
\end{theorem}
\begin{quotation}
New York Times: ,,Short-Cut to long proofs``.
\end{quotation}
Anwendung auf den Spannbaum.
Eingabe $x$ mit Beweis $\Pi$ seien $2^100$ bzw. $2^80$ Bit groß. Diese Eingabe passt in keinen Speicher.
Nach dem Theorem reicht es $100$ bzw. $80$ Bits und eine konstante Menge an zufälligen Bits zu lesen!


\section{Diskrete Wahrscheinlichkeitsverteilung}
Seien $(\Omega, \Sigma,\mathbb P)$ ein Wahrscheinlichkeitsraum. Man nennt $\mathbb P$ auch \Begriff{Wahrscheinlichkeitsverteilung} %
Die Begriffe Wahrscheinlichkeitsverteilung und Wahrscheinlichkeitsmaß werden synonym verwendet.

\subsection{Die Binomialverteilung}
Sei $\Omega = \{0,1,2, ..., n \}$ . Sei $0 \leq p \leq 1$. Für $\omega \in \Omega$ setzen wir 
\begin{align*}
p(\omega) &:= \binom{n}{k} \cdot p^k \cdot (1-p)^(n-k) 
\end{align*}

$\vec{p}(p(0),p(1),p(2),...,p(n))$ heißt der \Begriff{stochastische Vektor}. Sei $\Sigma := \mathcal P(\Omega)$ und definiere %
ein Wahrscheinlichkeitsmaß auf $\Sigma$ wie folgt: 
Für alle $A \in \Sigma$ sei
\begin{align*}
B_{(n,p)}(A) &:= \sum\limits_{\omega \in A} p(\omega) \\
	&= \sum\limits_{\omega \in A} \binom{n}{\omega}p^\omega(1-p)^{n-\omega}
\end{align*}
eine Funktion $B_{(n,p)}: \Sigma \rightarrow \lbrack 0,1 \rbrack$

\begin{theorem}
$B_{(n,p)}$ ist ein Wahrscheinlichkeitsmaß.
\end{theorem}
\begin{proof} Es sind drei Eigenschaften zu zeigen:
\newline
\begin{enumerate}
	\item\emph{$B_{(n,p)}(\Omega) = 1$:}
\begin{align*}
B_{(n,p)}(\Omega) &= \sum\limits_{\omega \in \Omega}p(\omega) \\
	&= \sum\limits_{\omega =0}^n \binom{n}{\omega}p^\omega(1-p)^{n-\omega}
	&= (p + (1 - p))^n = 1^n = 1
\end{align*}
	\item\emph{$0 \leq B_{(n,p)}(A) \leq 1 \forall A \subseteq \Omega$:}
siehe vorheriger Punkt
	\item\emph{$\sigma$-Additivität:}
Für $A_1,A_2, ... \in \Sigma $ paarweise disjunkt gilt, da es höchstens $l \leq n+1$ nicht-leere Teilmengen von $\Omega$ %
gibt:
\begin{align*}
B_{(n,p)}\left (\bigcup\limits_{i=1}^\infty A_i \right) &= B_{(n,p)}\left (\bigcup\limits_{k=1}^{l} A_{i_k} \right) \\
A &= \bigcup\limits_{k=1}^{l} A_{i_k} \\
B_{(n,p)}(A) &= \sum\limits_{\omega \in A} p(\omega) \\
	&= \sum\limits_{\omega \in A_{i_1}} p(\omega) +  ... + \sum\limits_{\omega \in A_{i_l}} p(\omega) \\
	&= B_{(n,p)}(A_{i_1}) + ... + B_{(n,p)}(A_{i_l}) \\
	&= \sum\limits_{k=1}^{l} B_{(n,p)}(A_{i_k}) \\
	&= \sum\limits_{i=1}^{\infty} B_{(n,p)}(A_{i})
\end{align*}
\end{enumerate}
\end{proof}
Das Wahrscheinlichkeitsmaß $B_{(n,p)}$ nennt man \Begriff{Binomialverteilung}.
\begin{definition}
Sei $\Omega$ eine abzählbar endlichen oder unendlichen Menge. Eine Funktion $p:\Omega \rightarrow \lbrack 0,1 \rbrack$ heißt %
\Begriff{stochastischer Vektor} falls $\sum\limits_{\omega \in \Omega} p(\omega) = 1$.
\end{definition}
\begin{theorem}\label{theorem:EndlicherWahrscheinlichkeitsraum}
Sei $p$ ein stochastischer Vektor über $\Omega$. Dann ist $(\Omega, \Sigma, \mathbb P)$ mit
\begin{align*}
\Sigma &:= \mathcal P(\Omega) \\
\mathbb P&: \Sigma \rightarrow \lbrack 0,1 \rbrack \\
P(A) &:= \sum\limits_{\omega \in A} p(\omega) , A \in \Sigma
\end{align*}
ein Wahrscheinlichkeitsraum und $\mathbb P$ ein Wahrscheinlichkeitsmaß.
\end{theorem}
\begin{proof} Es sind drei Eigenschaften zu zeigen:
\newline
\begin{enumerate}
\item\emph{$\mathbb P(\Omega) = 1$:}
Nach Definition:
\begin{align*}
\mathbb P(\Omega) &= \sum\limits_{\omega \in \Omega}p(\omega) &= 1
\end{align*}
	\item\emph{$0 \leq B_{(n,p)}(A) \leq 1 \forall A \subseteq \Omega$:}
siehe vorheriger Punkt
	\item\emph{$\sigma$-Additivität:}
Sei $A_1,A_2, ... $ eine Folge aus $\Sigma = \mathcal P(\Omega)$ von paarweise disjunkten Mengen. Dann gilt nach Definition %
von $p$:
\begin{align*}
\mathbb P\left ( \bigcup\limits_{i=1}^\infty A_i \right ) &= \sum\limits_{\omega \in A_1}p(\omega) + \sum\limits_{\omega \in A_1}p(\omega) + ... \\
	&= \mathbb P(A_1) + \mathbb P(A_2) + ... \\
	&= \sum\limits_{i=1}^\infty P(A_i)
\end{align*}
\end{enumerate}
\end{proof}

\subsection{Konstruktionsprinzip für endliche Wahrscheinlichkeitsmaße}
\begin{enumerate}
\item $\Omega$ endlich gegeben
\item Definiere $p(\omega), \omega \in \Omega$.
\item Zeige $\sum\limits_{\omega \in \Omega} p(\omega) = 1$.
\end{enumerate}
Dann erhalten wir mit Satz \ref{EndlicherWahrscheinlichkeitsraum} einen endlichen Wahrscheinlichkeitsraum $(\Omega,\mathcal P(\Omega),\mathbb P)$.
Dies gilt auch für abzählbar unendliche $\Omega$.

\subsection{Bernoulli-Verteilung}
$\Omega = \{0,1\}$, $\Sigma = \mathcal P(\Omega)$ und $p(\omega):=p^{\omega} (1-p)^{1-\omega}, \omega \in \Omega$ mit 
